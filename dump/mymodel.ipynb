{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, LSTM\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\korad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\korad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('time_series.csv')\n",
    "df.rename(columns={\n",
    "    'date': 'Date',\n",
    "    '1. open': 'Open',\n",
    "    '2. high': 'High',\n",
    "    '3. low': 'Low',\n",
    "    '4. close': 'Close',\n",
    "    '5. volume': 'Volume'\n",
    "}, inplace=True)\n",
    "# Sort by date in case the data isn't in order\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "\n",
    "labelanswer = df[0:1]\n",
    "df=df[1:]\n",
    "df = df.sort_values('Date')\n",
    "# Feature selection\n",
    "data = df[['Open', 'High', 'Low']]\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Preparing training data\n",
    "X_train = []\n",
    "y_train_high = []\n",
    "y_train_low = []\n",
    "\n",
    "for i in range(60, len(scaled_data)):\n",
    "    X_train.append(scaled_data[i-60:i, 0])  # Using Open prices for the last 60 days\n",
    "    y_train_high.append(scaled_data[i, 1])  # High price as target\n",
    "    y_train_low.append(scaled_data[i, 2])   # Low price as target\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train_high = np.array(y_train_high)\n",
    "y_train_low = np.array(y_train_low)\n",
    "\n",
    "# Reshaping for LSTM\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Building the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(units=25))\n",
    "\n",
    "# Two separate outputs for high and low prices\n",
    "model.add(Dense(units=2))  \n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, np.column_stack((y_train_high, y_train_low)), epochs=10, batch_size=32)\n",
    "def predict_high_low(open_price, previous_data):\n",
    "    # Create a copy of the previous data and add today's open price (without high and low yet)\n",
    "    test_data = np.vstack((previous_data, [open_price, np.nan, np.nan]))\n",
    "\n",
    "    # Scaling the data (ignore 'High' and 'Low' columns, as they are unknown for today's prediction)\n",
    "    test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "    # We only want the Open prices as input for prediction (the first column)\n",
    "    test_data_scaled_input = test_data_scaled[-60:, 0].reshape(1, -1, 1)\n",
    "\n",
    "    # Predict High and Low using the model\n",
    "    predicted_high_low_scaled = model.predict(test_data_scaled_input)\n",
    "\n",
    "    # Since the prediction output is scaled, we need to inverse transform it\n",
    "    # We create a dummy array for inverse transformation\n",
    "    predicted_high_low = scaler.inverse_transform([[open_price, predicted_high_low_scaled[0][0], predicted_high_low_scaled[0][1]]])\n",
    "\n",
    "    # Return the predicted High and Low values\n",
    "    return predicted_high_low[0][1], predicted_high_low[0][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timestamp('2024-09-09 00:00:00') 416.95 418.95 411.15 417.85 339508]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted High: 424.66066412776706\n",
      "Predicted Low: 405.7799095153809\n",
      "        Date    Open    High     Low   Close  Volume\n",
      "0 2024-09-09  416.95  418.95  411.15  417.85  339508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\korad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Let's assume today's open price is 150.0\n",
    "labelanswerdata = labelanswer.to_numpy()[0]\n",
    "todays_open = labelanswerdata[1]\n",
    "\n",
    "# Get the last 60 days' data (Open, High, Low) from the dataset\n",
    "previous_data = df[['Open', 'High', 'Low']].values[-60:]\n",
    "\n",
    "# Run the prediction\n",
    "predicted_high, predicted_low = predict_high_low(todays_open, previous_data)\n",
    "\n",
    "print(f\"Predicted High: {predicted_high}\")\n",
    "print(f\"Predicted Low: {predicted_low}\")\n",
    "print(labelanswer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('time_series.csv')\n",
    "df.rename(columns={\n",
    "    'date': 'Date',\n",
    "    '1. open': 'Open',\n",
    "    '2. high': 'High',\n",
    "    '3. low': 'Low',\n",
    "    '4. close': 'Close',\n",
    "    '5. volume': 'Volume'\n",
    "}, inplace=True)\n",
    "\n",
    "# Sort by date in case the data isn't in order\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Separate label for today's data\n",
    "labelanswer = df[0:1]\n",
    "df = df[1:]\n",
    "\n",
    "# Feature selection\n",
    "data = df[['Open', 'High', 'Low']]\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Preparing training data\n",
    "X_train = []\n",
    "y_train_high = []\n",
    "y_train_low = []\n",
    "\n",
    "for i in range(60, len(scaled_data)):\n",
    "    X_train.append(scaled_data[i-60:i, 0])  # Using Open prices for the last 60 days\n",
    "    y_train_high.append(scaled_data[i, 1])  # High price as target\n",
    "    y_train_low.append(scaled_data[i, 2])   # Low price as target\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train_high = np.array(y_train_high)\n",
    "y_train_low = np.array(y_train_low)\n",
    "\n",
    "# Reshaping for LSTM\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Building the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(units=25))\n",
    "model.add(Dense(units=2))  # Two separate outputs for high and low prices\n",
    "\n",
    "# Compile the model with a smaller learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "# Early stopping to prevent overfitting (increase patience if needed)\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, np.column_stack((y_train_high, y_train_low)), epochs=2000, batch_size=32, callbacks=[early_stopping])\n",
    "model.fit(X_train, np.column_stack((y_train_high, y_train_low)), epochs=2000, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Prediction function\n",
    "def predict_high_low(open_price, previous_data):\n",
    "    # Append today's open price (High and Low are unknown)\n",
    "    test_data = np.vstack((previous_data, [open_price, 0, 0]))  # Replace NaN with 0 for scaling\n",
    "\n",
    "    test_data_scaled = scaler.transform(test_data)\n",
    "    test_data_scaled_input = test_data_scaled[-60:, 0].reshape(1, -1, 1)\n",
    "    \n",
    "    # Predict High and Low\n",
    "    predicted_high_low_scaled = model.predict(test_data_scaled_input)\n",
    "    \n",
    "    # Inverse scaling (replace dummy values with predicted ones in the High and Low columns)\n",
    "    dummy_row = np.zeros((1, 3))  # 3 is for Open, High, and Low\n",
    "    dummy_row[0, 0] = open_price  # First column is Open\n",
    "    dummy_row[0, 1] = predicted_high_low_scaled[0][0]  # Second column is predicted High\n",
    "    dummy_row[0, 2] = predicted_high_low_scaled[0][1]  # Third column is predicted Low\n",
    "\n",
    "    predicted_high_low = scaler.inverse_transform(dummy_row)\n",
    "\n",
    "    return predicted_high_low[0][1], predicted_high_low[0][2]  # Return predicted High and Low values\n",
    "\n",
    "# Let's assume today's open price\n",
    "labelanswerdata = labelanswer.to_numpy()[0]\n",
    "todays_open = labelanswerdata[1]\n",
    "\n",
    "# Get the last 60 days' data (Open, High, Low) from the dataset\n",
    "previous_data = df[['Open', 'High', 'Low']].values[-60:]\n",
    "\n",
    "# Run the prediction\n",
    "predicted_high, predicted_low = predict_high_low(todays_open, previous_data)\n",
    "\n",
    "# Compare with true values\n",
    "true_high = labelanswerdata[2]\n",
    "true_low = labelanswerdata[3]\n",
    "\n",
    "print(f\"Predicted High: {predicted_high} | True High: {true_high}\")\n",
    "print(f\"Predicted Low: {predicted_low} | True Low: {true_low}\")\n",
    "\n",
    "# Calculate error metrics\n",
    "if not np.isnan(predicted_high) and not np.isnan(predicted_low):\n",
    "    mse_high = mean_squared_error([true_high], [predicted_high])\n",
    "    mse_low = mean_squared_error([true_low], [predicted_low])\n",
    "    mae_high = mean_absolute_error([true_high], [predicted_high])\n",
    "    mae_low = mean_absolute_error([true_low], [predicted_low])\n",
    "\n",
    "    print(f\"MSE High: {mse_high}, MSE Low: {mse_low}\")\n",
    "    print(f\"MAE High: {mae_high}, MAE Low: {mae_low}\")\n",
    "else:\n",
    "    print(\"Invalid prediction: NaN encountered.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
